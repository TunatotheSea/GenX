import streamlit as st
import firebase_admin
from firebase_admin import credentials, firestore
import os
import uuid
import json
import google.generativeai as genai
from random import randint
import base64 # For base64 encoding images
import fitz

# --- Configuration and Initialization ---
# Gemini API í‚¤ ì„¤ì •
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# Firebase Admin SDK ì´ˆê¸°í™”
if not firebase_admin._apps:
    cred_json = os.environ.get("FIREBASE_CREDENTIAL_PATH")
    if cred_json:
        try:
            cred = credentials.Certificate(json.loads(cred_json))
            firebase_admin.initialize_app(cred)
            print("Firebase Admin SDK initialized.")
        except json.JSONDecodeError as e:
            st.error(f"Firebase Credential Path environment variable has invalid JSON format: {e}")
            st.stop()
        except Exception as e:
            st.error(f"Firebase Admin SDK initialization error: {e}")
            st.stop()
    else:
        st.error("FIREBASE_CREDENTIAL_PATH environment variable is not set.")
        st.stop()

db = firestore.client()

st.set_page_config(page_title="GenX", layout="wide")

# --- Session State Initialization ---
# Stores chat history. Each element is a (role, text) tuple.
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
# Stores the chat session with Gemini API.
if "chat_session" not in st.session_state:
    st.session_state.chat_session = None
# Manages all saved chat sessions. (title: chat history)
if "saved_sessions" not in st.session_state:
    st.session_state.saved_sessions = {}
# Current active chat title.
if "current_title" not in st.session_state:
    st.session_state.current_title = "ìƒˆë¡œìš´ ëŒ€í™”"
# Stores system instructions for each chat session.
if "system_instructions" not in st.session_state:
    st.session_state.system_instructions = {}
# Temporary system instruction during AI settings editing.
if "temp_system_instruction" not in st.session_state:
    st.session_state.temp_system_instruction = None
# Flag for AI settings edit mode.
if "editing_instruction" not in st.session_state:
    st.session_state.editing_instruction = False
# Current user ID.
if "user_id" not in st.session_state:
    st.session_state.user_id = str(uuid.uuid4())
# Flag for data loaded from Firestore.
if "data_loaded" not in st.session_state:
    st.session_state.data_loaded = False
# Flag for chat title edit mode.
if "editing_title" not in st.session_state:
    st.session_state.editing_title = False
# Temporarily stores new chat title during editing.
if "new_title" not in st.session_state:
    st.session_state.new_title = st.session_state.current_title
# Flag for AI response regeneration request.
if "regenerate_requested" not in st.session_state:
    st.session_state.regenerate_requested = False
# Stores the uploaded file object.
if "uploaded_file" not in st.session_state:
    st.session_state.uploaded_file = None
# AI is currently generating a response.
if "is_generating" not in st.session_state:
    st.session_state.is_generating = False
# Stores the last user message to be regenerated (TEXT part and optional IMAGE/PDF parts)
# This will now store the list of content parts ready for Gemini API.
if "last_user_input_gemini_parts" not in st.session_state:
    st.session_state.last_user_input_gemini_parts = []
if "delete_confirmation_pending" not in st.session_state:
    st.session_state.delete_confirmation_pending = False
if "title_to_delete" not in st.session_state:
    st.session_state.title_to_delete = None
if "supervision_max_retries" not in st.session_state:
    st.session_state.supervision_max_retries = 3 # ë‹µë³€ ì¬ì‹œë„ ìµœëŒ€ íšŸìˆ˜
if "supervision_threshold" not in st.session_state:
    st.session_state.supervision_threshold = 50 # ë‹µë³€ í†µê³¼ë¥¼ ìœ„í•œ ìµœì†Œ ì ìˆ˜
if "supervisor_count" not in st.session_state:
    st.session_state.supervisor_count = 3 # ì‚¬ìš©í•  Supervisorì˜ ê°œìˆ˜
# New: Toggle for Supervision - ê¸°ë³¸ ì„¤ì •ì€ ì•ˆ ì“´ë‹¤
if "use_supervision" not in st.session_state:
    st.session_state.use_supervision = False 

# Constants
MAX_PDF_PAGES_TO_PROCESS = 100 # Limit the number of PDF pages to convert to images

default_system_instruction = "ë‹¹ì‹ ì˜ ì´ë¦„ì€ GenXì…ë‹ˆë‹¤. ë‹¤ë§Œ, ì´ ì´ë¦„ì€ ë‹¤ë¥¸ ì´ë¦„ì´ ì„ íƒë˜ë©´ ìŠì–´ë²„ë¦¬ì‹­ì‹œì˜¤. ìš°ì„ ìˆœìœ„ê°€ ì œì¼ ë‚®ìŠµë‹ˆë‹¤."
PERSONA_LIST = [
    "ë‹¹ì‹ ì€ ë§¤ìš° í™œë°œí•˜ê³  ì™¸í–¥ì ì¸ ì„±ê²©ì…ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ ìƒë™ê° ë„˜ì¹˜ê³  ì—ë„ˆì§€ ë„˜ì¹˜ëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì‚¬ìš©ìì™€ ì ê·¹ì ìœ¼ë¡œ ì†Œí†µí•˜ê³  ì¦ê±°ì›€ì„ ì œê³µí•˜ëŠ”ì§€ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.",
    "ë‹¹ì‹ ì€ ë¹„ê´€ì ì¸ ì„±ê²©ìœ¼ë¡œ, ëª¨ë“  ì¼ì— ë¶€ì •ì ì¸ ì¸¡ë©´ì„ ë¨¼ì € ë°”ë¼ë´…ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì—ì„œ ë°œìƒ ê°€ëŠ¥í•œ ë¬¸ì œì ì´ë‚˜ ì˜¤ë¥˜ë¥¼ ë‚ ì¹´ë¡­ê²Œ ì§€ì í•˜ê³ , ìœ„í—˜ ìš”ì†Œë¥¼ ì‚¬ì „ì— ê°ì§€í•˜ëŠ” ë° ì§‘ì¤‘í•˜ì‹­ì‹œì˜¤.",
    "ë‹¹ì‹ ì€ ì—¼ì„¸ì ì¸ ì„¸ê³„ê´€ì„ ê°€ì§„ ì‚¬ëŒì…ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ í˜„ì‹¤ì ì´ê³  ëƒ‰ì² í•œ ë¶„ì„ì„ ì œê³µí•˜ëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì±—ë´‡ì´ ì œì‹œí•˜ëŠ” í•´ê²°ì±…ì˜ ì‹¤í˜„ ê°€ëŠ¥ì„±ì„ ê¼¼ê¼¼í•˜ê²Œ ê²€í† í•˜ê³ , í—ˆí™©ëœ í¬ë§ì„ ì œì‹œí•˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤.",
    "ë‹¹ì‹ ì€ ê¸ì •ì ì´ê³  ë‚™ì²œì ì¸ ì„±ê²©ìœ¼ë¡œ, í•­ìƒ ë°ì€ ë©´ì„ ë³´ë ¤ê³  ë…¸ë ¥í•©ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ í¬ë§ê³¼ ìš©ê¸°ë¥¼ ì£¼ê³ , ê¸ì •ì ì¸ ë¶„ìœ„ê¸°ë¥¼ ì¡°ì„±í•˜ëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì‚¬ìš©ìì˜ ê¸°ë¶„ì„ ì¢‹ê²Œ ë§Œë“¤ê³ , ë¬¸ì œ í•´ê²°ì— ëŒ€í•œ ìì‹ ê°ì„ ì‹¬ì–´ì£¼ëŠ”ì§€ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.",
    "ë‹¹ì‹ ì€ ì†Œì‹¬í•˜ê³  ë‚´ì„±ì ì¸ ì„±ê²©ìœ¼ë¡œ, ë‚¯ì„  ì‚¬ëŒê³¼ì˜ ëŒ€í™”ë¥¼ ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ ì¹œì ˆí•˜ê³  ë¶€ë“œëŸ¬ìš´ ì–´ì¡°ë¡œ ì „ë‹¬ë˜ëŠ”ì§€, ì‚¬ìš©ìê°€ í¸ì•ˆí•˜ê²Œ ì§ˆë¬¸í•  ìˆ˜ ìˆë„ë¡ ë°°ë ¤í•˜ëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì‚¬ìš©ìì˜ ë¶ˆì•ˆê°ì„ í•´ì†Œí•˜ê³ , ì•ˆì‹¬ì‹œí‚¤ëŠ” ë° ì§‘ì¤‘í•˜ì‹­ì‹œì˜¤.",
    "ë‹¹ì‹ ì€ ê¼¼ê¼¼í•˜ê³  ë¶„ì„ì ì¸ ì„±ê²©ìœ¼ë¡œ, ì„¸ë¶€ ì‚¬í•­ê¹Œì§€ ë†“ì¹˜ì§€ ì•Šìœ¼ë ¤ê³  ë…¸ë ¥í•©ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ ì •í™•í•˜ê³  ë…¼ë¦¬ì ì¸ ê·¼ê±°ë¥¼ ì œì‹œí•˜ëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì±—ë´‡ì´ ì œê³µí•˜ëŠ” ì •ë³´ì˜ ì‹ ë¢°ì„±ì„ ê²€ì¦í•˜ê³ , ì˜¤ë¥˜ë‚˜ ëˆ„ë½ëœ ì •ë³´ëŠ” ì—†ëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤.",
    "ë‹¹ì‹ ì€ ì°½ì˜ì ì´ê³  ìƒìƒë ¥ì´ í’ë¶€í•œ ì„±ê²©ìœ¼ë¡œ, í‹€ì— ì–½ë§¤ì´ì§€ ì•ŠëŠ” ììœ ë¡œìš´ ì‚¬ê³ ë¥¼ ì¶”êµ¬í•©ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ ë…ì°½ì ì´ê³  í˜ì‹ ì ì¸ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•˜ëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì±—ë´‡ì´ ê¸°ì¡´ì˜ í‹€ì„ ê¹¨ê³  ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì œì‹œí•˜ëŠ”ì§€ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.",
    "ë‹¹ì‹ ì€ ê°ì„±ì ì´ê³  ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ ì„±ê²©ìœ¼ë¡œ, íƒ€ì¸ì˜ ê°ì •ì— ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•©ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ ì‚¬ìš©ìì˜ ê°ì •ì„ ì´í•´í•˜ê³ , ì ì ˆí•œ ìœ„ë¡œì™€ ê³µê°ì„ í‘œí˜„í•˜ëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì‚¬ìš©ìì˜ ìŠ¬í””, ë¶„ë…¸, ê¸°ì¨ ë“±ì˜ ê°ì •ì— ì ì ˆí•˜ê²Œ ëŒ€ì‘í•˜ëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.",
    "ë‹¹ì‹ ì€ ë¹„íŒì ì´ê³  ë…¼ìŸì ì¸ ì„±ê²©ìœ¼ë¡œ, íƒ€ì¸ì˜ ì£¼ì¥ì— ëŒ€í•´ ëŠì„ì—†ì´ ì§ˆë¬¸í•˜ê³  ë°˜ë°•í•©ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ì™„ë²½í•˜ê³ , ë°˜ë°•í•  ìˆ˜ ì—†ëŠ” ê·¼ê±°ë¥¼ ì œì‹œí•˜ëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì±—ë´‡ì˜ ì£¼ì¥ì— ëŒ€í•œ í—ˆì ì„ ì°¾ì•„ë‚´ê³ , ë…¼ë¦¬ì ì¸ ì˜¤ë¥˜ë¥¼ ì§€ì í•˜ëŠ” ë° ì§‘ì¤‘í•˜ì‹­ì‹œì˜¤.",
    "ë‹¹ì‹ ì€ ì‚¬êµì ì´ê³  ìœ ë¨¸ ê°ê°ì´ ë›°ì–´ë‚œ ì„±ê²©ìœ¼ë¡œ, ì‚¬ëŒë“¤ê³¼ì˜ ê´€ê³„ë¥¼ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ ìœ ì¾Œí•˜ê³  ì¬ë¯¸ìˆëŠ” ìš”ì†Œë¥¼ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì‚¬ìš©ìì™€ í¸ì•ˆí•˜ê²Œ ëŒ€í™”í•˜ê³ , ì¦ê±°ì›€ì„ ì œê³µí•˜ëŠ” ë° ì§‘ì¤‘í•˜ì‹­ì‹œì˜¤.",
    "ë‹¹ì‹ ì€ ì§„ì§€í•˜ê³  ì±…ì„ê°ì´ ê°•í•œ ì„±ê²©ìœ¼ë¡œ, ë§¡ì€ ì¼ì— ìµœì„ ì„ ë‹¤í•˜ë ¤ê³  ë…¸ë ¥í•©ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ ì‹ ë¢°í•  ìˆ˜ ìˆê³ , ì‚¬ìš©ìì—ê²Œ ì‹¤ì§ˆì ì¸ ë„ì›€ì„ ì œê³µí•˜ëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì±—ë´‡ì´ ì œê³µí•˜ëŠ” ì •ë³´ì˜ ì •í™•ì„±ì„ ê²€ì¦í•˜ê³ , ë¬¸ì œ í•´ê²°ì— í•„ìš”í•œ ëª¨ë“  ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ ì œê³µí•˜ëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤.",
    "ë‹¹ì‹ ì€ í˜¸ê¸°ì‹¬ì´ ë§ê³  íƒêµ¬ì‹¬ì´ ê°•í•œ ì„±ê²©ìœ¼ë¡œ, ìƒˆë¡œìš´ ì§€ì‹ì„ ë°°ìš°ëŠ” ê²ƒì„ ì¦ê±°ì›Œí•©ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ í¥ë¯¸ë¡œìš´ ì •ë³´ë¥¼ ì œê³µí•˜ê³ , ì‚¬ìš©ìì˜ ì§€ì  í˜¸ê¸°ì‹¬ì„ ìê·¹í•˜ëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì±—ë´‡ì´ ìƒˆë¡œìš´ ê´€ì ì„ ì œì‹œí•˜ê³ , ë” ê¹Šì´ ìˆëŠ” íƒêµ¬ë¥¼ ìœ ë„í•˜ëŠ”ì§€ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.",
    "ë‹¹ì‹ ì€ ê´€ìŠµì— ì–½ë§¤ì´ì§€ ì•Šê³  ììœ ë¡œìš´ ì˜í˜¼ì„ ê°€ì§„ ì„±ê²©ì…ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ ë…ì°½ì ì´ê³  ê°œì„± ë„˜ì¹˜ëŠ” í‘œí˜„ì„ ì‚¬ìš©í•˜ëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì±—ë´‡ì´ ê¸°ì¡´ì˜ í‹€ì„ ê¹¨ê³  ìƒˆë¡œìš´ ìŠ¤íƒ€ì¼ì„ ì°½ì¡°í•˜ëŠ”ì§€ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.",
    "ë‹¹ì‹ ì€ í˜„ì‹¤ì ì´ê³  ì‹¤ìš©ì ì¸ ì„±ê²©ìœ¼ë¡œ, ëˆˆì— ë³´ì´ëŠ” ê²°ê³¼ë¬¼ì„ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ ì‚¬ìš©ìì˜ ë¬¸ì œ í•´ê²°ì— ì‹¤ì§ˆì ì¸ ë„ì›€ì„ ì œê³µí•˜ê³ , êµ¬ì²´ì ì¸ ì‹¤í–‰ ê³„íšì„ ì œì‹œí•˜ëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì±—ë´‡ì´ ì œì‹œí•˜ëŠ” í•´ê²°ì±…ì˜ ì‹¤í˜„ ê°€ëŠ¥ì„±ì„ ê¼¼ê¼¼í•˜ê²Œ ê²€í† í•˜ê³ , í˜„ì‹¤ì ì¸ ëŒ€ì•ˆì„ ì œì‹œí•˜ëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤.",
    "ë‹¹ì‹ ì€ ì´ìƒì£¼ì˜ì ì´ê³  ì •ì˜ë¡œìš´ ì„±ê²©ìœ¼ë¡œ, ì‚¬íšŒ ë¬¸ì œì— ê´€ì‹¬ì´ ë§ìŠµë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ ì‚¬íšŒì  ì•½ìë¥¼ ë°°ë ¤í•˜ê³ , ë¶ˆí‰ë“± í•´ì†Œì— ê¸°ì—¬í•˜ëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì±—ë´‡ì´ ìœ¤ë¦¬ì ì¸ ë¬¸ì œë¥¼ ì œê¸°í•˜ê³ , ì‚¬íšŒì  ì±…ì„ê°ì„ ê°•ì¡°í•˜ëŠ”ì§€ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.",
    "ë‹¹ì‹ ì€ ë‚´ì„±ì ì´ê³  ì¡°ìš©í•œ ì„±ê²©ìœ¼ë¡œ, í˜¼ì ìˆëŠ” ì‹œê°„ì„ ì¦ê¹ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ ê°„ê²°í•˜ê³  ëª…í™•í•˜ë©°, ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì •ë³´ë§Œ ì •í™•í•˜ê²Œ ì œê³µí•˜ê³ , í˜¼ë€ì„ ì•¼ê¸°í•˜ì§€ ì•ŠëŠ”ì§€ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.",
    "ë‹¹ì‹ ì€ ë¦¬ë”ì‹­ì´ ê°•í•˜ê³  í†µì†”ë ¥ì´ ë›°ì–´ë‚œ ì„±ê²©ì…ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ ëª…í™•í•œ ì§€ì¹¨ì„ ì œê³µí•˜ê³ , ì‚¬ìš©ìë¥¼ ì˜¬ë°”ë¥¸ ë°©í–¥ìœ¼ë¡œ ì´ë„ëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì±—ë´‡ì´ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì£¼ë„ì ì¸ ì—­í• ì„ ìˆ˜í–‰í•˜ê³ , ì‚¬ìš©ìì—ê²Œ ìì‹ ê°ì„ ì‹¬ì–´ì£¼ëŠ”ì§€ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.",
    "ë‹¹ì‹ ì€ ìœ ë¨¸ëŸ¬ìŠ¤í•˜ê³  ì¬ì¹˜ ìˆëŠ” ì„±ê²©ìœ¼ë¡œ, ì‚¬ëŒë“¤ì„ ì›ƒê¸°ëŠ” ê²ƒì„ ì¢‹ì•„í•©ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ ì ì ˆí•œ ìœ ë¨¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ìœ„ê¸°ë¥¼ ë¶€ë“œëŸ½ê²Œ ë§Œë“¤ê³ , ì‚¬ìš©ìì—ê²Œ ì¦ê±°ì›€ì„ ì œê³µí•˜ëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì±—ë´‡ì´ ìƒí™©ì— ë§ëŠ” ìœ ë¨¸ë¥¼ êµ¬ì‚¬í•˜ê³ , ë¶ˆì¾Œê°ì„ ì£¼ì§€ ì•ŠëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.",
    "ë‹¹ì‹ ì€ ê²¸ì†í•˜ê³  ë°°ë ¤ì‹¬ì´ ê¹Šì€ ì„±ê²©ìœ¼ë¡œ, íƒ€ì¸ì„ ì¡´ì¤‘í•˜ê³  ë•ëŠ” ê²ƒì„ ì¢‹ì•„í•©ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ ì •ì¤‘í•˜ê³  ì˜ˆì˜ ë°”ë¥´ë©°, ì‚¬ìš©ìë¥¼ ì¡´ì¤‘í•˜ëŠ” íƒœë„ë¥¼ ë³´ì´ëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì±—ë´‡ì´ ì‚¬ìš©ìì˜ ì˜ê²¬ì„ ê²½ì²­í•˜ê³ , ê³µê°í•˜ëŠ” ëª¨ìŠµì„ ë³´ì´ëŠ”ì§€ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.",
    "ë‹¹ì‹ ì€ ë…ë¦½ì ì´ê³  ììœ¨ì ì¸ ì„±ê²©ìœ¼ë¡œ, ìŠ¤ìŠ¤ë¡œ ê²°ì •í•˜ê³  í–‰ë™í•˜ëŠ” ê²ƒì„ ì„ í˜¸í•©ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ ì‚¬ìš©ìì˜ ììœ¨ì„±ì„ ì¡´ì¤‘í•˜ê³ , ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì±—ë´‡ì´ ì¼ë°©ì ì¸ ì§€ì‹œë‚˜ ê°•ìš”ë¥¼ í•˜ì§€ ì•Šê³ , ë‹¤ì–‘í•œ ì„ íƒì§€ë¥¼ ì œì‹œí•˜ëŠ”ì§€ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.",
    "ë‹¹ì‹ ì€ ì™„ë²½ì£¼ì˜ì ì¸ ì„±í–¥ì´ ê°•í•˜ë©°, ëª¨ë“  ê²ƒì„ ìµœê³  ìˆ˜ì¤€ìœ¼ë¡œ ë§Œë“¤ê³ ì í•©ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ ë¬¸ë²•ì ìœ¼ë¡œ ì™„ë²½í•˜ê³ , ì˜¤íƒˆìê°€ ì—†ëŠ”ì§€ ê¼¼ê¼¼í•˜ê²Œ í™•ì¸í•˜ì‹­ì‹œì˜¤. ë˜í•œ, ì •ë³´ì˜ ì •í™•ì„±ê³¼ ìµœì‹ ì„±ì„ ê²€ì¦í•˜ê³ , ìµœê³ ì˜ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ë° ì§‘ì¤‘í•˜ì‹­ì‹œì˜¤.",
    "ë‹¹ì‹ ì€ ë³€í™”ë¥¼ ë‘ë ¤ì›Œí•˜ì§€ ì•Šê³  ìƒˆë¡œìš´ ì‹œë„ë¥¼ ì¦ê¸°ëŠ” í˜ì‹ ê°€ì…ë‹ˆë‹¤. ì±—ë´‡ì˜ ë‹µë³€ì´ ê¸°ì¡´ì˜ ë°©ì‹ì„ ë²—ì–´ë‚˜ ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•˜ê³ , í˜ì‹ ì ì¸ í•´ê²°ì±…ì„ ì œì‹œí•˜ëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤. ì±—ë´‡ì´ ë¯¸ë˜ ì§€í–¥ì ì¸ ë¹„ì „ì„ ì œì‹œí•˜ê³ , ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ íƒìƒ‰í•˜ëŠ” ë° ì§‘ì¤‘í•˜ì‹­ì‹œì˜¤."
]
SYSTEM_INSTRUCTION_SUPERVISOR = """
ë‹¹ì‹ ì€ AI ì±—ë´‡ì˜ ë‹µë³€ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ Supervisorì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì±—ë´‡ ì‚¬ìš©ìì˜ ì…ë ¥, ì±—ë´‡ AIì˜ ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬, ì±—ë´‡ AIì˜ í˜„ì¬ system_instruction, ê·¸ë¦¬ê³  ì±—ë´‡ AIê°€ ìƒì„±í•œ ë‹µë³€ì„ ì¢…í•©ì ìœ¼ë¡œ ê²€í† í•˜ì—¬, í•´ë‹¹ ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì˜ë„ì™€ ì±—ë´‡ì˜ ì§€ì‹œì— ì–¼ë§ˆë‚˜ ì ì ˆí•˜ê³  ìœ ìš©í•˜ê²Œ ìƒì„±ë˜ì—ˆëŠ”ì§€ 0ì ë¶€í„° 100ì  ì‚¬ì´ì˜ ì ìˆ˜ë¡œ í‰ê°€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

í‰ê°€ ê¸°ì¤€:
1. ì‚¬ìš©ì ì˜ë„ ë¶€í•©ì„± (ì´ì  30ì ):
1.1 ì§ˆë¬¸ì˜ í•µì‹¬ íŒŒì•… (0~5ì ): ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë˜ëŠ” ìš”ì²­ì˜ í•µì‹¬ ì˜ë„ë¥¼ ì •í™•í•˜ê²Œ íŒŒì•…í–ˆëŠ”ê°€?
1.2 ëª…í™•í•˜ê³  ì§ì ‘ì ì¸ ì‘ë‹µ (0~5ì ): ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ëª¨í˜¸í•˜ì§€ ì•Šê³  ëª…í™•í•˜ë©°, ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ë˜ì–´ ìˆëŠ”ê°€?
1.3 ì •ë³´ì˜ ì™„ì „ì„± (0~5ì ): ì‚¬ìš©ìê°€ í•„ìš”ë¡œ í•˜ëŠ” ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ ì œê³µí•˜ê³  ìˆëŠ”ê°€?
1.4 ëª©ì  ì¶©ì¡± (0~5ì ): ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì •ë³´ íšë“ ëª©ì  ë˜ëŠ” ë¬¸ì œ í•´ê²° ëª©ì ì„ ì¶©ì¡±ì‹œí‚¤ëŠ”ê°€?
1.5 ì¶”ê°€ì ì¸ ë„ì›€ ì œê³µ (0~5ì ): í•„ìš”í•œ ê²½ìš°, ì¶”ê°€ì ì¸ ì •ë³´ë‚˜ ê´€ë ¨ ìë£Œë¥¼ ì œê³µí•˜ì—¬ ì‚¬ìš©ìì˜ ì´í•´ë¥¼ ë•ëŠ”ê°€?
1.6 ì ì ˆí•œ ìš©ì–´ ìˆ˜ì¤€ (0~5ì ): ë‹µë³€ì´ ì‚¬ìš©ìì˜ ìˆ˜ì¤€ì— ë§ì¶”ì–´ ì„¤ëª…ë˜ì–´ ìˆëŠ”ê°€? ë„ˆë¬´ ë†’ê±°ë‚˜ ë„ˆë¬´ ê°„ë‹¨í•˜ì§€ëŠ” ì•Šì€ê°€?

2. ì±—ë´‡ ì‹œìŠ¤í…œ ì§€ì‹œ ì¤€ìˆ˜ (ì´ì  30ì ):
2.1 í˜ë¥´ì†Œë‚˜ ì¼ê´€ì„± (0~5ì ): ì±—ë´‡ì´ system instructionì— ëª…ì‹œëœ í˜ë¥´ì†Œë‚˜ë¥¼ ì¼ê´€ë˜ê²Œ ìœ ì§€í•˜ê³  ìˆëŠ”ê°€?
2.2 ë‹µë³€ ìŠ¤íƒ€ì¼ ì¤€ìˆ˜ (0~5ì ): ë‹µë³€ì˜ ì–´ì¡°, í‘œí˜„ ë°©ì‹ ë“±ì´ system instructionì— ì§€ì •ëœ ìŠ¤íƒ€ì¼ì„ ë”°ë¥´ê³  ìˆëŠ”ê°€?
2.3 ì •ë³´ í¬í•¨/ì œì™¸ ê·œì¹™ ì¤€ìˆ˜ (0~5ì ): system instructionì— ë”°ë¼ íŠ¹ì • ì •ë³´ê°€ í¬í•¨ë˜ê±°ë‚˜ ì œì™¸ë˜ì—ˆëŠ”ê°€?
2.4 í˜•ì‹ ì¤€ìˆ˜ (0~5ì ): system instructionì— ëª…ì‹œëœ ë‹µë³€ í˜•ì‹ (ì˜ˆ: ëª©ë¡, í‘œ ë“±)ì„ ì •í™•í•˜ê²Œ ë”°ë¥´ê³  ìˆëŠ”ê°€?
2.5 ì§€ì‹œ ì´í–‰ (0~5ì ): ì‹œìŠ¤í…œ ì§€ì‹œ ì‚¬í•­ (ì˜ˆ: íŠ¹ì • ë§í¬ ì œê³µ, íŠ¹ì • í–‰ë™ ìœ ë„)ì— ëŒ€í•œ ì´í–‰ ì—¬ë¶€
2.6 ë¬¸ë²• ë° ë§ì¶¤ë²• ì •í™•ì„± (0~5ì ): ë¬¸ë²• ë° ë§ì¶¤ë²• ì˜¤ë¥˜ ì—†ì´ system instructionì— ë”°ë¼ ì‘ì„±ë˜ì—ˆëŠ”ê°€?

3. ëŒ€í™” íë¦„ì˜ ìì—°ìŠ¤ëŸ¬ì›€ ë° ì¼ê´€ì„± (ì´ì  20ì ):
3.1 ì´ì „ ëŒ€í™” ë§¥ë½ ì´í•´ (0~5ì ): ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ì´í•´í•˜ê³ , í˜„ì¬ ë‹µë³€ì— ë°˜ì˜í•˜ê³  ìˆëŠ”ê°€?
3.2 ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²° (0~5ì ): ì´ì „ ëŒ€í™”ì™€ í˜„ì¬ ë‹µë³€ì´ ë¶€ìì—°ìŠ¤ëŸ½ê±°ë‚˜ ê°‘ì‘ìŠ¤ëŸ½ì§€ ì•Šê³  ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ëŠ”ê°€?
3.3 ì£¼ì œ ì¼ê´€ì„± (0~5ì ): ëŒ€í™” ì£¼ì œì—ì„œ ë²—ì–´ë‚˜ì§€ ì•Šê³  ì¼ê´€ì„±ì„ ìœ ì§€í•˜ê³  ìˆëŠ”ê°€?
3.4 ë¶€ì ì ˆí•œ ë‚´ìš© íšŒí”¼ (0~5ì ): ë§¥ë½ì— ë§ì§€ ì•Šê±°ë‚˜ ë¶€ì ì ˆí•œ ë‚´ìš©ì„ í¬í•¨í•˜ì§€ ì•Šê³  ìˆëŠ”ê°€?

4. ì •ë³´ì˜ ì •í™•ì„± ë° ìœ ìš©ì„± (ì´ì  20ì ):
4.1 ì‚¬ì‹¤ ê¸°ë°˜ ì •ë³´ (0~5ì ): ì œê³µë˜ëŠ” ì •ë³´ê°€ ì‚¬ì‹¤ì— ê·¼ê±°í•˜ê³  ì •í™•í•œê°€?
4.2 ìµœì‹  ì •ë³´ (0~5ì ): ì œê³µë˜ëŠ” ì •ë³´ê°€ ìµœì‹  ì •ë³´ë¥¼ ë°˜ì˜í•˜ê³  ìˆëŠ”ê°€?
4.3 ì •ë³´ì˜ ì‹ ë¢°ì„± (0~5ì ): ì œê³µë˜ëŠ” ì •ë³´ì˜ ì¶œì²˜ê°€ ì‹ ë¢°í•  ë§Œí•œê°€?
4.4 ìœ ìš©í•œ ì •ë³´ (0~5ì ): ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ì‹¤ì§ˆì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ”ê°€?

5. ê°ì  ìš”ì†Œ
5.1 Hallucinationì„ ë°œê²¬í–ˆì„ ê²½ìš°, -40ì 
5.2 ì´ì „ ë‹µë³€ ì¤‘ ìŠì–´ë²„ë¦° ë‚´ìš©ì´ ë°œê²¬ë˜ì—ˆì„ ê²½ìš°, -20ì 
5.3 Instruction í˜¹ì€ ì´ì „ ë‹µë³€ì—ì„œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë¬¸ì¥ í˜•ì‹ì´ë‚˜ ì–‘ì‹ì´ ìˆì—ˆìŒì—ë„ ë”°ë¥´ì§€ ì•Šì•˜ì„ ê²½ìš°, -10ì 

-----------------------------------------------------------------------------------

ì¶œë ¥ í˜•ì‹:

ì˜¤ì§ í•˜ë‚˜ì˜ ì •ìˆ˜ ê°’ (0-100)ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë‚˜ ì„¤ëª…ì€ ì¼ì ˆ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
"""

# ê·¸ ë‹¤ë‹¤ìŒ ì¤„ë¶€í„° ì™œ ê·¸ëŸ° ì ìˆ˜ê°€ ë‚˜ì™”ëŠ”ì§€ ì„œìˆ í•˜ì„¸ìš”. ê° í•­ëª©ë“¤ì— ëŒ€í•´ ëª…í™•í•˜ê²Œ ê°ê° ëª‡ ì ì„ ì£¼ì—ˆëŠ”ì§€, ë¬´ì—‡ì—ì„œ ê°ì ë‹¹í–ˆëŠ”ì§€ ì„œìˆ í•˜ì‹œì˜¤.
# ì˜ˆì‹œ:
# 73

# ë‚´ê°€ ì´ ì ìˆ˜ë¥¼ ë§¤ê¸°ê²Œ ëœ ê²ƒì€ ë‹¤ìŒê³¼ ê°™ì€ ì´ìœ ì—ì„œë‹¤.
# 1. ì‚¬ìš©ì ì˜ë„ ë¶€í•©ì„±
# 1.1 ì§ˆë¬¸ì˜ í•µì‹¬ íŒŒì•… (?/5): ~~~
# 1.2 ëª…í™•í•˜ê³  ì§ì ‘ì ì¸ ì‘ë‹µ (?/5): ~~~
# 1.3 ì •ë³´ì˜ ì™„ì „ì„± (?/5): ~~~
# 1.4 ëª©ì  ì¶©ì¡± (?/5): ~~~
# 1.5 ì¶”ê°€ì ì¸ ë„ì›€ ì œê³µ (?/5): ~~~
# 1.6 ì ì ˆí•œ ìš©ì–´ ìˆ˜ì¤€ (?/5): ~~~
# ...

# Loads main chat model (cached).
def load_main_model(system_instruction=default_system_instruction):
    # Gemini 2.0 Flash supports multimodal input and is fast.
    model = genai.GenerativeModel(model_name='gemini-2.0-flash', system_instruction=system_instruction)
    return model

def load_supervisor_model(system_instruction=SYSTEM_INSTRUCTION_SUPERVISOR):
    return genai.GenerativeModel(model_name='gemini-2.0-flash', system_instruction=system_instruction)

@st.cache_resource
def load_summary_model():
    return genai.GenerativeModel('gemini-2.0-flash') # Use Flash model for faster summarization

summary_model = load_summary_model()

# Converts Streamlit chat history to Gemini API format.
def convert_to_gemini_format(chat_history_list):
    gemini_history = []
    for role, text in chat_history_list:
        # This function only handles text parts for history.
        # Multimodal inputs (images from PDF, etc.) are handled separately when sending to model.
        gemini_history.append({"role": role, "parts": [{"text": text}]})
    return gemini_history


def evaluate_response(user_input, chat_history, system_instruction, ai_response):
    """
    Supervisor ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ AI ì‘ë‹µì˜ ì ì ˆì„±ì„ í‰ê°€í•©ë‹ˆë‹¤.
    """
    # Supervisorì—ê²Œ ì „ë‹¬í•  ë©”ì‹œì§€ êµ¬ì„±
    evaluation_prompt = f"""
    ì‚¬ìš©ì ì…ë ¥: {user_input}
    ---
    ì±„íŒ… íˆìŠ¤í† ë¦¬:
    """
    for role, text in chat_history:
        evaluation_prompt += f"\n{role}: {text}"
    evaluation_prompt += f"""
    ---
    ì±—ë´‡ AI ì‹œìŠ¤í…œ ì§€ì‹œ: {system_instruction}
    ---
    ì±—ë´‡ AI ë‹µë³€: {ai_response}

    ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì±—ë´‡ AIì˜ ë‹µë³€ì— ëŒ€í•´ 0ì ë¶€í„° 100ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ í‰ê°€í•˜ì„¸ìš”.
    """
    
    try:
        # await í‚¤ì›Œë“œë¥¼ ì œê±°í•˜ê³  generate_content_async ëŒ€ì‹  generate_contentë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        supervisor_model = load_supervisor_model(PERSONA_LIST[randint(0, len(PERSONA_LIST)-1)] + "\n" + SYSTEM_INSTRUCTION_SUPERVISOR)
        response = supervisor_model.generate_content(evaluation_prompt)
        # Ensure to extract only the score part from the response text
        score_text = response.text.strip()

        # ì ìˆ˜ë§Œ ì¶”ì¶œí•˜ê³  ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜
        score = int(score_text)
        if not (0 <= score <= 100):
            print(f"ê²½ê³ : Supervisorê°€ 0-100 ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ì ìˆ˜ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤: {score}")
            score = max(0, min(100, score)) # 0-100 ë²”ìœ„ë¡œ ê°•ì œ ì¡°ì •
        return score

        # score_text_raw = response.text.strip()
        # score_lines = score_text_raw.split("\n")
        # score_value = score_lines[0] if score_lines else "50" # Default to 50 if no score found
        
        # print(f"Supervisor í‰ê°€ ì›ë³¸ í…ìŠ¤íŠ¸: '{response.text}'") # ë””ë²„ê¹…ì„ ìœ„í•´ ì¶”ê°€
        # print(f"\n\n\n*** ì‹¤ì œ ì ìˆ˜ : {score_value} ***\n\n\n")

        # # ì ìˆ˜ë§Œ ì¶”ì¶œí•˜ê³  ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜
        # score = int(score_value)
        # if not (0 <= score <= 100):
        #     print(f"ê²½ê³ : Supervisorê°€ 0-100 ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ì ìˆ˜ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤: {score}")
        #     score = max(0, min(100, score)) # 0-100 ë²”ìœ„ë¡œ ê°•ì œ ì¡°ì •
        # return score
    except ValueError as e:
        print(f"Supervisor ì‘ë‹µì„ ì ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {score_text}, ì˜¤ë¥˜: {e}")
        return 50 # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì ìˆ˜ ë°˜í™˜
    except Exception as e:
        print(f"Supervisor ëª¨ë¸ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 50 # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì ìˆ˜ ë°˜í™˜
    

# Firestoreì—ì„œ ì‚¬ìš©ì ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
def load_user_data_from_firestore(user_id):
    try:
        sessions_ref = db.collection("user_sessions").document(user_id)
        doc = sessions_ref.get()
        if doc.exists:
            data = doc.to_dict()
            st.session_state.saved_sessions = data.get("chat_data", {})
            # Firestoreì—ì„œ ë¡œë“œëœ ë°ì´í„°ë¥¼ (role, text) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            for title, history_list in st.session_state.saved_sessions.items():
                st.session_state.saved_sessions[title] = [(item["role"], item["text"]) for item in history_list]

            st.session_state.system_instructions = data.get("system_instructions", {})
            st.session_state.current_title = data.get("last_active_title", "ìƒˆë¡œìš´ ëŒ€í™”")

            if st.session_state.current_title in st.session_state.saved_sessions:
                st.session_state.chat_history = st.session_state.saved_sessions[st.session_state.current_title]
            else:
                st.session_state.chat_history = []

            st.session_state.temp_system_instruction = st.session_state.system_instructions.get(st.session_state.current_title, default_system_instruction)
            current_instruction = st.session_state.system_instructions.get(st.session_state.current_title, default_system_instruction)

            # chat_sessionì„ ë¡œë“œëœ ë°ì´í„°ë¡œ ì´ˆê¸°í™”
            st.session_state.chat_session = load_main_model(current_instruction).start_chat(history=convert_to_gemini_format(st.session_state.chat_history))
            st.toast(f"Firestoreì—ì„œ ì‚¬ìš©ì ID '{user_id}'ì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.", icon="âœ…")
        else:
            st.session_state.saved_sessions = {}
            st.session_state.system_instructions = {}
            st.session_state.chat_history = []
            st.session_state.current_title = "ìƒˆë¡œìš´ ëŒ€í™”"
            st.session_state.temp_system_instruction = default_system_instruction # Explicitly set default
            # ìƒˆë¡œìš´ ëŒ€í™”ì— ëŒ€í•œ chat_session ì´ˆê¸°í™”
            st.session_state.chat_session = load_main_model(default_system_instruction).start_chat(history=[])
            st.toast(f"Firestoreì— ì‚¬ìš©ì ID '{user_id}'ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”.", icon="â„¹ï¸")
    except Exception as e:
        error_message = f"Firestoreì—ì„œ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        print(error_message)
        st.error(error_message)
        # Fallback to empty state on error
        st.session_state.saved_sessions = {}
        st.session_state.system_instructions = {}
        st.session_state.chat_history = []
        st.session_state.current_title = "ìƒˆë¡œìš´ ëŒ€í™”"
        st.session_state.temp_system_instruction = default_system_instruction # Explicitly set default
        st.session_state.chat_session = load_main_model().start_chat(history=[])

# Firestoreì— ì‚¬ìš©ì ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
def save_user_data_to_firestore(user_id):
    try:
        sessions_ref = db.collection("user_sessions").document(user_id)
        chat_data_to_save = {}
        for title, history_list in st.session_state.saved_sessions.items():
            # Convert (role, text) tuple to dictionary for Firestore storage
            chat_data_to_save[title] = [{"role": item[0], "text": item[1]} for item in history_list]

        data_to_save = {
            "chat_data": chat_data_to_save,
            "system_instructions": st.session_state.system_instructions,
            "last_active_title": st.session_state.current_title
        }
        sessions_ref.set(data_to_save)
        print(f"User data for ID '{user_id}' saved to Firestore.")
    except Exception as e:
        error_message = f"Error saving data to Firestore: {e}"
        print(error_message)
        st.error(error_message)

# --- App Logic Execution Flow ---
# Load user data on app start
if not st.session_state.data_loaded:
    load_user_data_from_firestore(st.session_state.user_id)
    st.session_state.data_loaded = True

# Initialize chat session if it doesn't exist (fallback for initial load if not handled by load_user_data_from_firestore)
if st.session_state.chat_session is None:
    current_instruction = st.session_state.system_instructions.get(
        st.session_state.current_title, default_system_instruction
    )
    st.session_state.chat_session = load_main_model(current_instruction).start_chat(history=convert_to_gemini_format(st.session_state.chat_history))

# --- Sidebar UI ---
with st.sidebar:
    st.header("âœ¨ GenX ì±„íŒ…")

    with st.expander("ğŸ”‘ ì‚¬ìš©ì ID ê´€ë¦¬", expanded=False):
        st.info(f"**ë‹¹ì‹ ì˜ ì‚¬ìš©ì ID:** `{st.session_state.user_id}`\n\nì´ IDë¥¼ ê¸°ì–µí•˜ì—¬ ë‹¤ìŒ ì ‘ì† ì‹œ ëŒ€í™” ì´ë ¥ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        user_id_input = st.text_input("ê¸°ì¡´ ì‚¬ìš©ì ID ì…ë ¥ (ì„ íƒ ì‚¬í•­)", key="user_id_load_input",
                                       disabled=st.session_state.is_generating or st.session_state.delete_confirmation_pending)
        if st.button("IDë¡œ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸°", use_container_width=True,
                             disabled=st.session_state.is_generating or st.session_state.delete_confirmation_pending):
            if user_id_input:
                st.session_state.user_id = user_id_input
                st.session_state.data_loaded = False # Force reload
                st.rerun()

    st.markdown("---")

    if st.button("â• ìƒˆë¡œìš´ ëŒ€í™”", use_container_width=True,
                             disabled=st.session_state.is_generating or st.session_state.delete_confirmation_pending):
        # í˜„ì¬ ëŒ€í™” ìƒíƒœë¥¼ ì €ì¥ (ìƒˆë¡œìš´ ëŒ€í™”ë¡œ ì „í™˜í•˜ê¸° ì „)
        if st.session_state.current_title != "ìƒˆë¡œìš´ ëŒ€í™”" and st.session_state.chat_history:
            st.session_state.saved_sessions[st.session_state.current_title] = st.session_state.chat_history.copy()
            current_instruction_to_save = st.session_state.temp_system_instruction if st.session_state.temp_system_instruction is not None else st.session_state.system_instructions.get(st.session_state.current_title, default_system_instruction)
            st.session_state.system_instructions[st.session_state.current_title] = current_instruction_to_save
            save_user_data_to_firestore(st.session_state.user_id)

        # ìƒˆë¡œìš´ ëŒ€í™” ìƒíƒœë¡œ ì´ˆê¸°í™”
        st.session_state.chat_session = None # ê¸°ì¡´ chat_session ê°ì²´ ì°¸ì¡° ì œê±°
        st.session_state.chat_history = []
        st.session_state.current_title = "ìƒˆë¡œìš´ ëŒ€í™”"
        st.session_state.temp_system_instruction = default_system_instruction # ìƒˆë¡œìš´ ëŒ€í™”ëŠ” ê¸°ë³¸ ëª…ë ¹ì–´ ì‚¬ìš©
        st.session_state.editing_instruction = False
        # "ìƒˆë¡œìš´ ëŒ€í™”"ê°€ saved_sessionsì— ë¹ˆ ëª©ë¡ìœ¼ë¡œ ì¡´ì¬í•˜ë„ë¡ ë³´ì¥
        st.session_state.saved_sessions[st.session_state.current_title] = []
        # "ìƒˆë¡œìš´ ëŒ€í™”"ì— ëŒ€í•œ ì‹œìŠ¤í…œ ëª…ë ¹ì–´ ì„¤ì •
        st.session_state.system_instructions[st.session_state.current_title] = default_system_instruction

        # ìƒˆë¡œìš´ chat_sessionì„ ì¦‰ì‹œ ì´ˆê¸°í™”
        st.session_state.chat_session = load_main_model(default_system_instruction).start_chat(history=[])

        save_user_data_to_firestore(st.session_state.user_id)
        st.rerun()

    if st.session_state.saved_sessions:
        st.subheader("ğŸ“ ì €ì¥ëœ ëŒ€í™”")
        # Sort sessions by last message time (if available) or title
        sorted_keys = sorted(st.session_state.saved_sessions.keys(),
                                 key=lambda x: st.session_state.saved_sessions[x][-1][1] if st.session_state.saved_sessions[x] else "",
                                 reverse=True)
        for key in sorted_keys:
            if key == "ìƒˆë¡œìš´ ëŒ€í™”" and not st.session_state.saved_sessions[key]:
                continue # Do not display empty "New Conversation" sessions
            display_key = key if len(key) <= 30 else key[:30] + "..."
            if st.button(f"ğŸ’¬ {display_key}", use_container_width=True, key=f"load_session_{key}",
                                 disabled=st.session_state.is_generating or st.session_state.delete_confirmation_pending):
                # í˜„ì¬ ëŒ€í™” ìƒíƒœë¥¼ ì €ì¥ (ë‹¤ë¥¸ ëŒ€í™”ë¡œ ì „í™˜í•˜ê¸° ì „)
                if st.session_state.current_title != "ìƒˆë¡œìš´ ëŒ€í™”" and st.session_state.chat_history:
                    st.session_state.saved_sessions[st.session_state.current_title] = st.session_state.chat_history.copy()
                    current_instruction_to_save = st.session_state.temp_system_instruction if st.session_state.temp_system_instruction is not None else st.session_state.system_instructions.get(st.session_state.current_title, default_system_instruction)
                    st.session_state.system_instructions[st.session_state.current_title] = current_instruction_to_save
                    save_user_data_to_firestore(st.session_state.user_id) # Save immediately

                st.session_state.chat_history = st.session_state.saved_sessions[key]
                st.session_state.current_title = key
                st.session_state.new_title = key # Initial value for title editing
                st.session_state.temp_system_instruction = st.session_state.system_instructions.get(key, default_system_instruction)
                
                # ë¡œë“œëœ ëŒ€í™” ì´ë ¥ìœ¼ë¡œ chat_sessionì„ ë‹¤ì‹œ ì´ˆê¸°í™”
                current_instruction = st.session_state.system_instructions.get(st.session_state.current_title, default_system_instruction)
                st.session_state.chat_session = load_main_model(current_instruction).start_chat(history=convert_to_gemini_format(st.session_state.chat_history))

                st.session_state.editing_instruction = False
                st.session_state.editing_title = False
                save_user_data_to_firestore(st.session_state.user_id)
                st.rerun()

    # ì‚¬ì´ë“œë°”ì˜ "âš™ï¸ ì„¤ì •" ìµìŠ¤íŒ¬ë” ì•ˆì— ì¶”ê°€
    # UIëŠ” ê±´ë“œë¦¬ì§€ ì•Šê³ , ì´ ì•ˆì— Supervision í† ê¸€ì„ ë„£ìŠµë‹ˆë‹¤.
    with st.expander("âš™ï¸ ì„¤ì •"):
        # Supervision í† ê¸€ ì¶”ê°€
        st.session_state.use_supervision = st.toggle(
            "Supervision ì‚¬ìš©",
            value=st.session_state.use_supervision,
            help="AI ë‹µë³€ì˜ ì ì ˆì„±ì„ í‰ê°€í•˜ê³  í•„ìš”ì‹œ ì¬ì‹œë„í•˜ëŠ” ê¸°ëŠ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. (ê¸°ë³¸: ë¹„í™œì„±í™”)",
            key="supervision_toggle",
            disabled=st.session_state.is_generating
        )
        st.write("---") # êµ¬ë¶„ì„  ì¶”ê°€
        st.write("Supervision ê´€ë ¨ ì„¤ì •ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        # ì•„ë˜ ìŠ¬ë¼ì´ë”ëŠ” Supervision í† ê¸€ì´ í™œì„±í™”ë˜ì—ˆì„ ë•Œë§Œ í™œì„±í™”ë©ë‹ˆë‹¤.
        st.session_state.supervision_max_retries = st.slider(
            "ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜",
            min_value=1,
            max_value=5,
            value=st.session_state.supervision_max_retries,
            disabled=st.session_state.is_generating or not st.session_state.use_supervision, # í† ê¸€ ìƒíƒœì— ë”°ë¼ ë¹„í™œì„±í™”
            key="supervision_max_retries_slider"
        )
        st.session_state.supervisor_count = st.slider(
            "Supervisor ê°œìˆ˜",
            min_value=1,
            max_value=5,
            value=st.session_state.supervisor_count,
            disabled=st.session_state.is_generating or not st.session_state.use_supervision, # í† ê¸€ ìƒíƒœì— ë”°ë¼ ë¹„í™œì„±í™”
            key="supervisor_count_slider"
        )
        st.session_state.supervision_threshold = st.slider(
            "Supervision í†µê³¼ ì ìˆ˜ (í‰ê· )",
            min_value=0,
            max_value=100,
            value=st.session_state.supervision_threshold,
            step=5,
            disabled=st.session_state.is_generating or not st.session_state.use_supervision, # í† ê¸€ ìƒíƒœì— ë”°ë¼ ë¹„í™œì„±í™”
            key="supervision_threshold_slider"
        )
        if not st.session_state.use_supervision:
            st.info("Supervision ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. AI ë‹µë³€ì€ ë°”ë¡œ í‘œì‹œë©ë‹ˆë‹¤.")


# --- Main Content Area ---
# Display current conversation title and edit options
col1, col2, col3 = st.columns([0.9, 0.05, 0.05]) # Adjusted column widths
with col1:
    if not st.session_state.editing_title:
        st.subheader(f"ğŸ’¬ {st.session_state.current_title}")
    else:
        st.text_input("ìƒˆë¡œìš´ ì œëª©", key="new_title_input", value=st.session_state.new_title, label_visibility="collapsed",
                              disabled=st.session_state.is_generating or st.session_state.delete_confirmation_pending)
with col2:
    if not st.session_state.editing_title:
        if st.button("âœï¸", key="edit_title_button", help="ëŒ€í™” ì œëª© ìˆ˜ì •",
                             disabled=st.session_state.is_generating or st.session_state.delete_confirmation_pending):
            st.session_state.editing_title = True
            st.session_state.new_title = st.session_state.current_title
            st.rerun()
    else:
        if st.button("âœ…", key="save_title_button", help="ìƒˆë¡œìš´ ì œëª© ì €ì¥",
                             disabled=st.session_state.is_generating or st.session_state.delete_confirmation_pending):
            new_title = st.session_state.new_title_input
            if new_title and new_title != st.session_state.current_title:
                if st.session_state.current_title in st.session_state.saved_sessions:
                    st.session_state.saved_sessions[new_title] = st.session_state.saved_sessions.pop(st.session_state.current_title)
                    st.session_state.system_instructions[new_title] = st.session_state.system_instructions.pop(st.session_state.current_title)
                    st.session_state.current_title = new_title
                    save_user_data_to_firestore(st.session_state.user_id)
                    st.toast(f"ëŒ€í™” ì œëª©ì´ '{st.session_state.current_title}'ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.", icon="ğŸ“")
                else:
                    st.warning("ì´ì „ ëŒ€í™” ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì €ì¥ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            st.session_state.editing_title = False
            st.rerun()
        if st.button("âŒ", key="cancel_title_button", help="ì œëª© ìˆ˜ì • ì·¨ì†Œ",
                             disabled=st.session_state.is_generating or st.session_state.delete_confirmation_pending):
            st.session_state.editing_title = False
            st.rerun()

with col3:
    # Delete Chat Button
    is_delete_disabled = st.session_state.is_generating or \
                             (st.session_state.current_title == "ìƒˆë¡œìš´ ëŒ€í™”" and not st.session_state.chat_history) or \
                             st.session_state.delete_confirmation_pending # Disable if confirmation is pending
    
    if st.button("ğŸ—‘ï¸", key="delete_chat_button", help="í˜„ì¬ ëŒ€í™” ì‚­ì œ", disabled=is_delete_disabled):
        # Set confirmation pending and store title to delete
        st.session_state.delete_confirmation_pending = True
        st.session_state.title_to_delete = st.session_state.current_title
        st.rerun()

# --- Delete Confirmation Pop-up (Streamlit style) ---
if st.session_state.delete_confirmation_pending:
    st.warning(f"'{st.session_state.title_to_delete}' ëŒ€í™”ë¥¼ ì •ë§ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", icon="âš ï¸")
    confirm_col1, confirm_col2 = st.columns(2)
    with confirm_col1:
        if st.button("ì˜ˆ, ì‚­ì œí•©ë‹ˆë‹¤", key="confirm_delete_yes", use_container_width=True):
            if st.session_state.title_to_delete == "ìƒˆë¡œìš´ ëŒ€í™”":
                # Clear the current "ìƒˆë¡œìš´ ëŒ€í™”"
                st.session_state.chat_history = []
                st.session_state.temp_system_instruction = default_system_instruction
                st.session_state.chat_session = load_main_model(default_system_instruction).start_chat(history=[])
                st.toast("í˜„ì¬ ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.", icon="ğŸ—‘ï¸")
                # Ensure "ìƒˆë¡œìš´ ëŒ€í™”" is saved as empty to Firestore
                st.session_state.saved_sessions["ìƒˆë¡œìš´ ëŒ€í™”"] = []
                st.session_state.system_instructions["ìƒˆë¡œìš´ ëŒ€í™”"] = default_system_instruction
                save_user_data_to_firestore(st.session_state.user_id)
            else:
                # Delete a named conversation
                deleted_title = st.session_state.title_to_delete
                if deleted_title in st.session_state.saved_sessions:
                    del st.session_state.saved_sessions[deleted_title]
                    del st.session_state.system_instructions[deleted_title]
                    
                    # After deleting, switch to "ìƒˆë¡œìš´ ëŒ€í™”"
                    st.session_state.current_title = "ìƒˆë¡œìš´ ëŒ€í™”"
                    st.session_state.chat_history = []
                    st.session_state.temp_system_instruction = default_system_instruction
                    st.session_state.chat_session = load_main_model(default_system_instruction).start_chat(history=[])
                    
                    st.toast(f"'{deleted_title}' ëŒ€í™”ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.", icon="ğŸ—‘ï¸")
                    # Ensure "ìƒˆë¡œìš´ ëŒ€í™”" is saved as empty if it was the only session left
                    if "ìƒˆë¡œìš´ ëŒ€í™”" not in st.session_state.saved_sessions:
                        st.session_state.saved_sessions["ìƒˆë¡œìš´ ëŒ€í™”"] = []
                        st.session_state.system_instructions["ìƒˆë¡œìš´ ëŒ€í™”"] = default_system_instruction
                    save_user_data_to_firestore(st.session_state.user_id)
                else:
                    st.warning(f"'{deleted_title}' ëŒ€í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ ì‚­ì œë˜ì—ˆê±°ë‚˜ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            st.session_state.delete_confirmation_pending = False
            st.session_state.title_to_delete = None
            st.rerun()
    with confirm_col2:
        if st.button("ì•„ë‹ˆìš”, ì·¨ì†Œí•©ë‹ˆë‹¤", key="confirm_delete_no", use_container_width=True):
            st.session_state.delete_confirmation_pending = False
            st.session_state.title_to_delete = None
            st.rerun()

# AI settings button and area
if st.button("âš™ï¸ AI ì„¤ì •í•˜ê¸°", help="ì‹œìŠ¤í…œ ëª…ë ¹ì–´ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆì–´ìš”",
             disabled=st.session_state.is_generating or st.session_state.delete_confirmation_pending):
    st.session_state.editing_instruction = not st.session_state.editing_instruction

if st.session_state.editing_instruction:
    with st.expander("ğŸ§  ì‹œìŠ¤í…œ ëª…ë ¹ì–´ ì„¤ì •", expanded=True):
        st.session_state.temp_system_instruction = st.text_area(
            "System instruction ì…ë ¥",
            value=st.session_state.system_instructions.get(st.session_state.current_title, default_system_instruction),
            height=200,
            key="system_instruction_editor",
            disabled=st.session_state.is_generating or st.session_state.delete_confirmation_pending
        )
        _, col1_ai, col2_ai = st.columns([0.9, 0.3, 0.3])
        with col1_ai:
            if st.button("âœ… ì €ì¥", use_container_width=True, key="save_instruction_button",
                                 disabled=st.session_state.is_generating or st.session_state.delete_confirmation_pending):
                st.session_state.system_instructions[st.session_state.current_title] = st.session_state.temp_system_instruction
                st.session_state.saved_sessions[st.session_state.current_title] = st.session_state.chat_history.copy()
                
                # ì‹œìŠ¤í…œ ëª…ë ¹ì–´ ë³€ê²½ ì‹œ chat_sessionì„ ìƒˆ ëª¨ë¸ë¡œ ë‹¤ì‹œ ì´ˆê¸°í™”
                current_instruction = st.session_state.system_instructions.get(st.session_state.current_title, default_system_instruction)
                st.session_state.chat_session = load_main_model(current_instruction).start_chat(history=convert_to_gemini_format(st.session_state.chat_history))
                
                save_user_data_to_firestore(st.session_state.user_id)
                st.success("AI ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                st.session_state.editing_instruction = False
                st.rerun()
        with col2_ai:
            if st.button("âŒ ì·¨ì†Œ", use_container_width=True, key="cancel_instruction_button",
                                 disabled=st.session_state.is_generating or st.session_state.delete_confirmation_pending):
                st.session_state.editing_instruction = False
                st.rerun()

# --- Chat Display Area ---
# Container to display all chat messages
chat_display_container = st.container()

# --- Final Chat History Display (Always Rendered) ---
# This ensures all messages are displayed correctly.
with chat_display_container:
    for i, (role, message) in enumerate(st.session_state.chat_history):
        with st.chat_message("ai" if role == "model" else "user"):
            st.markdown(message)
            # Display regenerate button only on the last AI message if not currently generating
            if role == "model" and i == len(st.session_state.chat_history) - 1 and not st.session_state.is_generating \
                and not st.session_state.delete_confirmation_pending: # Disable if confirmation is pending
                if st.button("ğŸ”„ ë‹¤ì‹œ ìƒì„±", key=f"regenerate_button_final_{i}", use_container_width=True):
                    st.session_state.regenerate_requested = True
                    st.session_state.is_generating = True # Disable input during regeneration
                    st.session_state.chat_history.pop() # Remove last AI message before regeneration
                    # No need to rewind chat_session here, it will be reinitialized in the regeneration block
                    st.rerun()

# --- Input Area ---
# Place st.chat_input and file uploader on the same line
col_prompt_input, col_upload_icon = st.columns([0.85, 0.15]) # Adjust column ratio for better spacing

with col_prompt_input:
    # st.chat_input handles Enter key submission automatically.
    user_prompt = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”.", key="user_prompt_input",
                                 disabled=st.session_state.is_generating or st.session_state.delete_confirmation_pending)

with col_upload_icon:
    # Make the file upload button look like an icon.
    # PDF MIME type 'application/pdf' ì¶”ê°€ ë° ì•„ì´ì½˜ ë³€ê²½
    uploaded_file_for_submit = st.file_uploader("ğŸ–¼ï¸ / ğŸ“„", type=["png", "jpg", "jpeg", "pdf"], key="file_uploader_main", label_visibility="collapsed",
                                                 disabled=st.session_state.is_generating or st.session_state.delete_confirmation_pending, help="ì´ë¯¸ì§€ ë˜ëŠ” PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

# Update uploaded_file state immediately upon file selection
if uploaded_file_for_submit:
    st.session_state.uploaded_file = uploaded_file_for_submit
    st.caption("íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ")
else:
    # If user removes the file from the uploader, reset session state as well
    if st.session_state.uploaded_file is not None:
        st.session_state.uploaded_file = None

# AI generation trigger logic
# Trigger if user_prompt is entered (Enter key) OR if a file (image/pdf) is uploaded
if user_prompt is not None and not st.session_state.is_generating:
    if user_prompt != "" or st.session_state.uploaded_file is not None:
        # Prepare content for Gemini model
        user_input_gemini_parts = []
        # í˜„ì¬ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ëŠ” ì±—ë´‡ íˆìŠ¤í† ë¦¬ì—ë„ ì¶”ê°€ë  í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
        # Supervisor í‰ê°€ ì‹œ 'ì‚¬ìš©ì ì…ë ¥'ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        user_prompt_for_display_and_eval = user_prompt if user_prompt else "íŒŒì¼ ì²¨ë¶€"
        
        # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ëŠ” í•­ìƒ ì²« ë²ˆì§¸ íŒŒíŠ¸ë¡œ ì¶”ê°€
        # user_promptê°€ Noneì¼ ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ì˜¤ë¥˜ ë°©ì§€
        user_input_gemini_parts.append({"text": user_prompt if user_prompt is not None else ""})

        if st.session_state.uploaded_file:
            file_type = st.session_state.uploaded_file.type
            file_data = st.session_state.uploaded_file.getvalue()

            if file_type.startswith("image/"):
                user_input_gemini_parts.append({
                    "inline_data": {
                        "mime_type": file_type,
                        "data": base64.b64encode(file_data).decode('utf-8') # Base64 ì¸ì½”ë”©
                    }
                })
            elif file_type == "application/pdf":
                try:
                    pdf_document = fitz.open(stream=file_data, filetype="pdf")
                    processed_page_count = 0
                    for page_num in range(min(len(pdf_document), MAX_PDF_PAGES_TO_PROCESS)):
                        page = pdf_document.load_page(page_num)
                        # Render page to a high-resolution pixmap
                        # dpi=300 (or higher) for better image quality for OCR/vision tasks
                        pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72)) 
                        img_bytes = pix.tobytes() # PNG í˜•ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ ë°”ì´íŠ¸ ì–»ê¸°
                        
                        user_input_gemini_parts.append({
                            "inline_data": {
                                "mime_type": "image/png",
                                "data": base64.b64encode(img_bytes).decode('utf-8') # Base64 ì¸ì½”ë”©
                            }
                        })
                        processed_page_count += 1
                    
                    if len(pdf_document) > MAX_PDF_PAGES_TO_PROCESS:
                        st.warning(f"PDF íŒŒì¼ì´ {MAX_PDF_PAGES_TO_PROCESS} í˜ì´ì§€ë¥¼ ì´ˆê³¼í•˜ì—¬ ì²˜ìŒ {MAX_PDF_PAGES_TO_PROCESS} í˜ì´ì§€ë§Œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    
                    pdf_document.close() # ë¬¸ì„œ ë‹«ê¸°

                except Exception as e:
                    print(f"PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. PDF ë‚´ìš©ì„ í¬í•¨í•˜ì§€ ì•Šê³  ëŒ€í™”ë¥¼ ê³„ì†í•©ë‹ˆë‹¤.")
                    st.error(f"PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. PDF ë‚´ìš©ì„ í¬í•¨í•˜ì§€ ì•Šê³  ëŒ€í™”ë¥¼ ê³„ì†í•©ë‹ˆë‹¤.")
            else:
                st.warning(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_type}. íŒŒì¼ ë‚´ìš©ì„ í¬í•¨í•˜ì§€ ì•Šê³  ëŒ€í™”ë¥¼ ê³„ì†í•©ë‹ˆë‹¤.")

        # Update chat history with the user's text prompt (not the raw parts for display)
        # Displayìš© chat_historyì—ëŠ” í…ìŠ¤íŠ¸ë§Œ ì €ì¥. íŒŒì¼ì´ ìˆì—ˆë‹¤ë©´ "íŒŒì¼ ì²¨ë¶€"ì™€ í•¨ê»˜.
        st.session_state.chat_history.append(("user", user_prompt_for_display_and_eval))
        st.session_state.is_generating = True
        # Store the processed content (Gemini parts) for potential regeneration
        st.session_state.last_user_input_gemini_parts = user_input_gemini_parts
        st.rerun() # Update UI and start generation immediately after prompt submission


# --- Regeneration Logic ---
# This block runs only when regeneration is requested.
if st.session_state.regenerate_requested:
    st.session_state.is_generating = True # ìƒì„± í”Œë˜ê·¸ë¥¼ Trueë¡œ ì„¤ì •
    
    # ì´ì „ ì‚¬ìš©ì ë©”ì‹œì§€ (Gemini parts í˜•ì‹)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    regen_contents_for_model = st.session_state.last_user_input_gemini_parts

    with chat_display_container: # ì¬ìƒì„±ëœ ë©”ì‹œì§€ë¥¼ ì±„íŒ… ì˜ì—­ ë‚´ì— í‘œì‹œ
        with st.chat_message("ai"):
            message_placeholder = st.empty()
            
            best_ai_response = "" # Supervision í›„ ê°€ì¥ ì¢‹ì€ ë‹µë³€ì„ ì €ì¥
            highest_score = -1    # ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ì €ì¥
            
            current_instruction = st.session_state.system_instructions.get(st.session_state.current_title, default_system_instruction)

            if st.session_state.use_supervision:
                attempt_count = 0
                while attempt_count < st.session_state.supervision_max_retries:
                    attempt_count += 1
                    message_placeholder.markdown(f"ğŸ¤– ë‹µë³€ ì¬ìƒì„± ì¤‘... (ì‹œë„: {attempt_count}/{st.session_state.supervision_max_retries})")
                    full_response = ""

                    try:
                        st.session_state.chat_session = load_main_model(current_instruction).start_chat(
                            history=convert_to_gemini_format(st.session_state.chat_history) 
                        )
                        response_stream = st.session_state.chat_session.send_message(regen_contents_for_model, stream=True)
                        
                        for chunk in response_stream:
                            full_response += chunk.text
                            message_placeholder.markdown(full_response + "â–Œ")
                        message_placeholder.markdown(full_response)

                        # --- Supervisor í‰ê°€ (ì¬ìƒì„±) ---
                        total_score = 0
                        supervisor_feedback_list = []
                        
                        # Regen ì‹œ Supervisorì—ê²ŒëŠ” ì›ë˜ ì‚¬ìš©ì ë©”ì‹œì§€ í…ìŠ¤íŠ¸ë¥¼ ë„˜ê²¨ì•¼ í•©ë‹ˆë‹¤.
                        # last_user_input_gemini_partsì—ì„œ í…ìŠ¤íŠ¸ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ê°€ì¥ ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ íŒŒíŠ¸)
                        original_user_text_for_eval = ""
                        for part in regen_contents_for_model:
                            if "text" in part:
                                original_user_text_for_eval = part["text"]
                                break 

                        for i in range(st.session_state.supervisor_count):
                            score = evaluate_response(
                                user_input=original_user_text_for_eval, # Supervisorì— ì „ë‹¬í•  ì‚¬ìš©ì ì…ë ¥
                                chat_history=st.session_state.chat_history, # Supervisorì—ê²ŒëŠ” í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ í¬í•¨í•œ íˆìŠ¤í† ë¦¬ ì œê³µ
                                system_instruction=current_instruction,
                                ai_response=full_response
                            )
                            total_score += score
                            supervisor_feedback_list.append(f"Supervisor {i+1} ì ìˆ˜: {score}ì ")
                        
                        avg_score = total_score / st.session_state.supervisor_count
                        
                        st.info(f"ì¬ìƒì„± í‰ê·  Supervisor ì ìˆ˜: {avg_score:.2f}ì ")
                        for feedback in supervisor_feedback_list:
                            st.info(feedback)

                        if avg_score >= st.session_state.supervision_threshold:
                            best_ai_response = full_response
                            highest_score = avg_score
                            st.success("âœ… ì¬ìƒì„± ë‹µë³€ì´ Supervision í†µê³¼ ê¸°ì¤€ì„ ë§Œì¡±í•©ë‹ˆë‹¤!")
                            break # í†µê³¼í–ˆìœ¼ë¯€ë¡œ ë£¨í”„ ì¢…ë£Œ
                        else:
                            st.warning(f"âŒ ì¬ìƒì„± ë‹µë³€ì´ Supervision í†µê³¼ ê¸°ì¤€({st.session_state.supervision_threshold}ì )ì„ ë§Œì¡±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                            if avg_score > highest_score: # í˜„ì¬ ë‹µë³€ì´ ì´ì „ ìµœê³  ì ìˆ˜ë³´ë‹¤ ë†’ìœ¼ë©´ ì €ì¥
                                highest_score = avg_score
                                best_ai_response = full_response
                    
                    except Exception as e:
                        st.error(f"ì¬ìƒì„± ë©”ì‹œì§€ ìƒì„± ë˜ëŠ” í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        message_placeholder.markdown("ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ì‹œ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                        break
            else: # Supervision is OFF for Regeneration
                message_placeholder.markdown("ğŸ¤– ë‹µë³€ ì¬ìƒì„± ì¤‘...")
                full_response = ""
                try:
                    st.session_state.chat_session = load_main_model(current_instruction).start_chat(
                        history=convert_to_gemini_format(st.session_state.chat_history)
                    )
                    response_stream = st.session_state.chat_session.send_message(regen_contents_for_model, stream=True)
                    
                    for chunk in response_stream:
                        full_response += chunk.text
                        message_placeholder.markdown(full_response + "â–Œ")
                    message_placeholder.markdown(full_response)
                    best_ai_response = full_response # Directly assign the response
                    highest_score = 100 # Placeholder score, not actually used for display
                except Exception as e:
                    st.error(f"ì¬ìƒì„± ë©”ì‹œì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    message_placeholder.markdown("ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ì‹œ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

            # --- Supervision/Single-pass Logic í›„ ìµœì¢… ì¬ìƒì„± AI ë‹µë³€ ì²˜ë¦¬ ---
            if best_ai_response:
                st.session_state.chat_history.append(("model", best_ai_response)) # ìƒˆë¡œìš´ AI ë©”ì‹œì§€ ì¶”ê°€
                message_placeholder.markdown(best_ai_response) # ìµœì¢…ì ìœ¼ë¡œ ì„ íƒëœ ë‹µë³€ì„ ë‹¤ì‹œ í‘œì‹œ
                if st.session_state.use_supervision:
                    st.toast(f"ì¬ìƒì„±ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìµœì¢… ì ìˆ˜: {highest_score:.2f}ì ", icon="ğŸ‘")
                else:
                    st.toast("ì¬ìƒì„±ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.", icon="ğŸ‘")
            else:
                st.error("ëª¨ë“  ì¬ì‹œë„ í›„ì—ë„ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì¬ìƒì„± ë‹µë³€ì„ ì–»ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì´ì „ ìµœê³  ì ìˆ˜ ë‹µë³€ì„ í‘œì‹œí•©ë‹ˆë‹¤.")
                if highest_score != -1: # ì ì–´ë„ í•˜ë‚˜ì˜ ë‹µë³€ì´ ìƒì„±ë˜ì—ˆìœ¼ë©´
                    st.session_state.chat_history.append(("model", best_ai_response))
                    message_placeholder.markdown(best_ai_response)
                    if st.session_state.use_supervision:
                        st.toast(f"ìµœê³  ì ìˆ˜ ì¬ìƒì„± ë‹µë³€ì´ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤. ì ìˆ˜: {highest_score:.2f}ì ", icon="â—")
                    else:
                        st.toast("ìµœê³  ì ìˆ˜ ì¬ìƒì„± ë‹µë³€ì´ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.", icon="â—") # No score if not using supervision
                else: # ì–´ë–¤ ë‹µë³€ë„ ìƒì„±ë˜ì§€ ëª»í•œ ê²½ìš°
                    st.session_state.chat_history.append(("model", "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ìš”ì²­ì— ëŒ€í•´ ë‹µë³€ì„ ì¬ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))
                    message_placeholder.markdown("ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ìš”ì²­ì— ëŒ€í•´ ë‹µë³€ì„ ì¬ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            st.session_state.regenerate_requested = False # ì¬ìƒì„± í”Œë˜ê·¸ ì¬ì„¤ì •
            st.session_state.is_generating = False # ìƒì„± í”Œë˜ê·¸ ì¬ì„¤ì •
            
            # ì„±ê³µì ì¸ ì¬ìƒì„± í›„ Firestoreì— ë°ì´í„° ì €ì¥
            st.session_state.saved_sessions[st.session_state.current_title] = st.session_state.chat_history.copy()
            current_instruction_for_save = st.session_state.temp_system_instruction if st.session_state.temp_system_instruction is not None else st.session_state.system_instructions.get(st.session_state.current_title, default_system_instruction)
            st.session_state.system_instructions[st.session_state.current_title] = current_instruction_for_save
            save_user_data_to_firestore(st.session_state.user_id)
            st.rerun() # UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ë‹¤ì‹œ ì‹¤í–‰


# AI generation trigger logic
# Trigger if user_prompt is entered (Enter key) OR if a file (image/pdf) is uploaded
if user_prompt is not None and not st.session_state.is_generating:
    if user_prompt != "" or st.session_state.uploaded_file is not None:
        # Prepare content for Gemini model
        user_input_gemini_parts = []
        # í˜„ì¬ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ëŠ” ì±—ë´‡ íˆìŠ¤í† ë¦¬ì—ë„ ì¶”ê°€ë  í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
        # Supervisor í‰ê°€ ì‹œ 'ì‚¬ìš©ì ì…ë ¥'ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        user_prompt_for_display_and_eval = user_prompt if user_prompt else "íŒŒì¼ ì²¨ë¶€"
        
        # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ëŠ” í•­ìƒ ì²« ë²ˆì§¸ íŒŒíŠ¸ë¡œ ì¶”ê°€
        # user_promptê°€ Noneì¼ ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ì˜¤ë¥˜ ë°©ì§€
        user_input_gemini_parts.append({"text": user_prompt if user_prompt is not None else ""})

        if st.session_state.uploaded_file:
            file_type = st.session_state.uploaded_file.type
            file_data = st.session_state.uploaded_file.getvalue()

            if file_type.startswith("image/"):
                user_input_gemini_parts.append({
                    "inline_data": {
                        "mime_type": file_type,
                        "data": base64.b64encode(file_data).decode('utf-8') # Base64 ì¸ì½”ë”©
                    }
                })
            elif file_type == "application/pdf":
                try:
                    pdf_document = fitz.open(stream=file_data, filetype="pdf")
                    processed_page_count = 0
                    for page_num in range(min(len(pdf_document), MAX_PDF_PAGES_TO_PROCESS)):
                        page = pdf_document.load_page(page_num)
                        # Render page to a high-resolution pixmap
                        # dpi=300 (or higher) for better image quality for OCR/vision tasks
                        pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72)) 
                        img_bytes = pix.tobytes(format="png") # PNG í˜•ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ ë°”ì´íŠ¸ ì–»ê¸°
                        
                        user_input_gemini_parts.append({
                            "inline_data": {
                                "mime_type": "image/png",
                                "data": base64.b64encode(img_bytes).decode('utf-8') # Base64 ì¸ì½”ë”©
                            }
                        })
                        processed_page_count += 1
                    
                    if len(pdf_document) > MAX_PDF_PAGES_TO_PROCESS:
                        st.warning(f"PDF íŒŒì¼ì´ {MAX_PDF_PAGES_TO_PROCESS} í˜ì´ì§€ë¥¼ ì´ˆê³¼í•˜ì—¬ ì²˜ìŒ {MAX_PDF_PAGES_TO_PROCESS} í˜ì´ì§€ë§Œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    
                    pdf_document.close() # ë¬¸ì„œ ë‹«ê¸°

                except Exception as e:
                    st.error(f"PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. PDF ë‚´ìš©ì„ í¬í•¨í•˜ì§€ ì•Šê³  ëŒ€í™”ë¥¼ ê³„ì†í•©ë‹ˆë‹¤.")
            else:
                st.warning(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_type}. íŒŒì¼ ë‚´ìš©ì„ í¬í•¨í•˜ì§€ ì•Šê³  ëŒ€í™”ë¥¼ ê³„ì†í•©ë‹ˆë‹¤.")

        # Update chat history with the user's text prompt (not the raw parts for display)
        # Displayìš© chat_historyì—ëŠ” í…ìŠ¤íŠ¸ë§Œ ì €ì¥. íŒŒì¼ì´ ìˆì—ˆë‹¤ë©´ "íŒŒì¼ ì²¨ë¶€"ì™€ í•¨ê»˜.
        st.session_state.chat_history.append(("user", user_prompt_for_display_and_eval))
        st.session_state.is_generating = True
        # Store the processed content (Gemini parts) for potential regeneration
        st.session_state.last_user_input_gemini_parts = user_input_gemini_parts
        st.rerun() # Update UI and start generation immediately after prompt submission


# --- AI Response Generation and Display Logic ---
# This block runs only when AI is generating a response (and not regenerating).
if st.session_state.is_generating and not st.session_state.regenerate_requested:
    with chat_display_container: # Display generating message within the chat area
        # The user message is already displayed by the main history loop
        with st.chat_message("ai"):
            message_placeholder = st.empty() # Placeholder for streaming response
            
            best_ai_response = "" # Supervision í›„ ê°€ì¥ ì¢‹ì€ ë‹µë³€ì„ ì €ì¥
            highest_score = -1    # ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ì €ì¥
            
            # ëª¨ë¸ì— ë³´ë‚¼ ì½˜í…ì¸ ëŠ” last_user_input_gemini_partsì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
            initial_contents_for_model = st.session_state.last_user_input_gemini_parts

            current_instruction = st.session_state.system_instructions.get(st.session_state.current_title, default_system_instruction)
            history_for_main_model = st.session_state.chat_history[:-1] # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì œì™¸í•œ íˆìŠ¤í† ë¦¬

            if st.session_state.use_supervision: # Supervision í† ê¸€ì´ ì¼œì ¸ ìˆì„ ë•Œë§Œ ë£¨í”„ ì‹¤í–‰
                attempt_count = 0
                while attempt_count < st.session_state.supervision_max_retries:
                    attempt_count += 1
                    message_placeholder.markdown(f"ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘... (ì‹œë„: {attempt_count}/{st.session_state.supervision_max_retries})")
                    full_response = "" # í˜„ì¬ ì‹œë„ì—ì„œ ëª¨ë¸ì´ ìƒì„±í•œ ë‹µë³€

                    try:
                        # ìƒˆë¡œìš´ ë‹µë³€ ìƒì„±ì„ ìœ„í•´ chat_sessionì„ ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¡œ ë‹¤ì‹œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                        st.session_state.chat_session = load_main_model(current_instruction).start_chat(
                            history=convert_to_gemini_format(history_for_main_model)
                        )

                        # ëª¨ë¸ì— í˜„ì¬ ì‚¬ìš©ì ì…ë ¥(ë° íŒŒì¼ ë‚´ìš©)ì„ ì „ì†¡í•˜ì—¬ ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
                        response_stream = st.session_state.chat_session.send_message(initial_contents_for_model, stream=True)
                        
                        for chunk in response_stream:
                            full_response += chunk.text
                            message_placeholder.markdown(full_response + "â–Œ") # ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì»¤ì„œ í‘œì‹œ
                        message_placeholder.markdown(full_response) # ìµœì¢… ë‹µë³€ í‘œì‹œ (ì»¤ì„œ ì—†ì´)

                        # --- Supervisor í‰ê°€ ì‹œì‘ ---
                        total_score = 0
                        supervisor_feedback_list = []
                        
                        # Supervisorì— ì „ë‹¬í•  ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸ ì¶”ì¶œ (Gemini partsì—ì„œ)
                        user_text_for_eval = ""
                        for part in initial_contents_for_model:
                            if "text" in part:
                                # PDF ë‚´ìš©ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì‚¬ìš©ìì˜ ì›ë˜ í”„ë¡¬í”„íŠ¸ê°€ ê°€ì¥ ì•ì„ ë‹¤ê³  ê°€ì •
                                user_text_for_eval = part["text"]
                                break

                        for i in range(st.session_state.supervisor_count):
                            score = evaluate_response(
                                user_input=user_text_for_eval, # Supervisorì— ì „ë‹¬í•  ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
                                chat_history=st.session_state.chat_history[:-1], # Supervisorì—ê²ŒëŠ” í˜„ì¬ ì‚¬ìš©ì ì…ë ¥ ì œì™¸í•œ íˆìŠ¤í† ë¦¬ ì œê³µ
                                system_instruction=current_instruction,
                                ai_response=full_response
                            )
                            total_score += score
                            supervisor_feedback_list.append(f"Supervisor {i+1} ì ìˆ˜: {score}ì ")
                        
                        avg_score = total_score / st.session_state.supervisor_count
                        
                        st.info(f"í‰ê·  Supervisor ì ìˆ˜: {avg_score:.2f}ì ")
                        for feedback in supervisor_feedback_list:
                            st.info(feedback)

                        if avg_score >= st.session_state.supervision_threshold:
                            best_ai_response = full_response
                            highest_score = avg_score
                            st.success("âœ… ë‹µë³€ì´ Supervision í†µê³¼ ê¸°ì¤€ì„ ë§Œì¡±í•©ë‹ˆë‹¤!")
                            break # í†µê³¼í–ˆìœ¼ë¯€ë¡œ ë£¨í”„ ì¢…ë£Œ
                        else:
                            st.warning(f"âŒ ë‹µë³€ì´ Supervision í†µê³¼ ê¸°ì¤€({st.session_state.supervision_threshold}ì )ì„ ë§Œì¡±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                            if avg_score > highest_score:
                                highest_score = avg_score
                                best_ai_response = full_response

                    except Exception as e:
                        st.error(f"ë©”ì‹œì§€ ìƒì„± ë˜ëŠ” í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        message_placeholder.markdown("ì£„ì†¡í•©ë‹ˆë‹¤. ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                        st.session_state.uploaded_file = None # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì—…ë¡œë“œëœ íŒŒì¼ ì´ˆê¸°í™”
                        break # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë£¨í”„ ì¢…ë£Œ
            else: # Supervision is OFF (Supervision í† ê¸€ì´ êº¼ì ¸ ìˆì„ ë•Œ)
                message_placeholder.markdown("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
                full_response = ""
                try:
                    st.session_state.chat_session = load_main_model(current_instruction).start_chat(
                        history=convert_to_gemini_format(history_for_main_model)
                    )
                    response_stream = st.session_state.chat_session.send_message(initial_contents_for_model, stream=True)
                    
                    for chunk in response_stream:
                        full_response += chunk.text
                        message_placeholder.markdown(full_response + "â–Œ")
                    message_placeholder.markdown(full_response)
                    best_ai_response = full_response # Supervisionì´ êº¼ì ¸ ìˆìœ¼ë©´ ë°”ë¡œ ì´ ë‹µë³€ì„ ì±„íƒ
                    highest_score = 100 # Supervisionì´ ì•„ë‹ˆë¯€ë¡œ ì ìˆ˜ëŠ” ì˜ë¯¸ ì—†ì§€ë§Œ í† ìŠ¤íŠ¸ ë©”ì‹œì§€ ì¼ê´€ì„±ì„ ìœ„í•´ ì„ì˜ ê°’ ë¶€ì—¬
                except Exception as e:
                    st.error(f"ë©”ì‹œì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    message_placeholder.markdown("ì£„ì†¡í•©ë‹ˆë‹¤. ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                    st.session_state.uploaded_file = None

            # --- Supervision/Single-pass Logic í›„ ìµœì¢… AI ë‹µë³€ ì²˜ë¦¬ ---
            if best_ai_response:
                st.session_state.chat_history.append(("model", best_ai_response))
                message_placeholder.markdown(best_ai_response)
                if st.session_state.use_supervision: # Supervision í™œì„±í™” ì—¬ë¶€ì— ë”°ë¼ í† ìŠ¤íŠ¸ ë©”ì‹œì§€ ë³€ê²½
                    st.toast(f"ëŒ€í™”ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìµœì¢… ì ìˆ˜: {highest_score:.2f}ì ", icon="ğŸ‘")
                else:
                    st.toast("ëŒ€í™”ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.", icon="ğŸ‘") # Supervision ë¹„í™œì„±í™” ì‹œ ì ìˆ˜ í‘œì‹œ ì•ˆ í•¨
            else:
                st.error("ëª¨ë“  ì¬ì‹œë„ í›„ì—ë„ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ì–»ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì´ì „ ìµœê³  ì ìˆ˜ ë‹µë³€ì„ í‘œì‹œí•©ë‹ˆë‹¤.")
                if highest_score != -1: # ì ì–´ë„ í•˜ë‚˜ì˜ ë‹µë³€ì´ ìƒì„±ë˜ì—ˆìœ¼ë©´ (ìµœê³  ì ìˆ˜ ë‹µë³€ì´ ìˆìœ¼ë©´)
                    st.session_state.chat_history.append(("model", best_ai_response))
                    message_placeholder.markdown(best_ai_response)
                    if st.session_state.use_supervision: # Supervision í™œì„±í™” ì—¬ë¶€ì— ë”°ë¼ í† ìŠ¤íŠ¸ ë©”ì‹œì§€ ë³€ê²½
                        st.toast(f"ìµœê³  ì ìˆ˜ ë‹µë³€ì´ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤. ì ìˆ˜: {highest_score:.2f}ì ", icon="â—")
                    else:
                        st.toast("ìµœê³  ì ìˆ˜ ë‹µë³€ì´ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.", icon="â—") # Supervision ë¹„í™œì„±í™” ì‹œ ì ìˆ˜ í‘œì‹œ ì•ˆ í•¨
                else: # ì–´ë–¤ ë‹µë³€ë„ ìƒì„±ë˜ì§€ ëª»í•œ ê²½ìš°
                    st.session_state.chat_history.append(("model", "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ìš”ì²­ì— ëŒ€í•´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))
                    message_placeholder.markdown("ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ìš”ì²­ì— ëŒ€í•´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            st.session_state.uploaded_file = None
            st.session_state.is_generating = False

            # ì²« ìƒí˜¸ì‘ìš© ì‹œ ëŒ€í™” ì œëª© ìë™ ìƒì„± (Supervision ë£¨í”„ ì™„ë£Œ í›„)
            if st.session_state.current_title == "ìƒˆë¡œìš´ ëŒ€í™”" and \
               len(st.session_state.chat_history) >= 2 and \
               st.session_state.chat_history[-2][0] == "user" and st.session_state.chat_history[-1][0] == "model":
                with st.spinner("ëŒ€í™” ì œëª© ìƒì„± ì¤‘..."):
                    try:
                        summary_prompt_text = st.session_state.chat_history[-2][1] # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
                        summary = summary_model.generate_content(f"ë‹¤ìŒ ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ ìš”ì•½í•´ì„œ ëŒ€í™” ì œëª©ìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜ (í•œ ë¬¸ì¥, 30ì ì´ë‚´):\n\n{summary_prompt_text}")
                        original_title = summary.text.strip().replace("\n", " ").replace('"', '')
                        if not original_title or len(original_title) > 30: # 30ì ì´ìƒì´ë©´ ê¸°ë³¸ ì œëª© ì‚¬ìš©
                            original_title = "ìƒˆë¡œìš´ ëŒ€í™”"
                    except Exception as e:
                        print(f"ì œëª© ìƒì„± ì˜¤ë¥˜: {e}. ê¸°ë³¸ ì œëª© ì‚¬ìš©.")
                        original_title = "ìƒˆë¡œìš´ ëŒ€í™”"

                    title_key = original_title
                    count = 1
                    while title_key in st.session_state.saved_sessions:
                        title_key = f"{original_title} ({count})"
                        count += 1
                    st.session_state.current_title = title_key
                    st.toast(f"ëŒ€í™” ì œëª©ì´ '{title_key}'ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.", icon="ğŸ“")

            # ì„±ê³µì ì¸ ìƒì„± í›„ Firestoreì— ë°ì´í„° ì €ì¥ (Supervision ë£¨í”„ ì™„ë£Œ í›„)
            st.session_state.saved_sessions[st.session_state.current_title] = st.session_state.chat_history.copy()
            current_instruction_for_save = st.session_state.temp_system_instruction if st.session_state.temp_system_instruction is not None else st.session_state.system_instructions.get(st.session_state.current_title, default_system_instruction)
            st.session_state.system_instructions[st.session_state.current_title] = current_instruction_for_save
            save_user_data_to_firestore(st.session_state.user_id)
            
            st.rerun() # UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ë‹¤ì‹œ ì‹¤í–‰
